{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar correction - getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was the CoNLL-2013 Shared Task: Grammatical Error Correction.\n",
    "\n",
    "Copyright (C) 2013 Hwee Tou Ng, Joel Tetreault, Siew Mei Wu,\n",
    "                   Yuanbin Wu, Christian Hadiwinoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workfolder setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "data_folder = 'data'\n",
    "\n",
    "if os.path.exists(data_folder):\n",
    "    shutil.rmtree(data_folder, ignore_errors=True)\n",
    "\n",
    "\n",
    "# Data folder creation\n",
    "os.mkdir(data_folder)\n",
    "os.chdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd0d350437846f2822d03605bedb649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Archive decompression:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# URL link for the dataset\n",
    "remote_url_link = \"https://www.comp.nus.edu.sg/~nlp/conll13st/release2.3.1.tar.gz\"\n",
    "\n",
    "\n",
    "def download_file(url_link: str) -> str:\n",
    "    \"\"\" Download the archive file and returns the local filename\n",
    "    for decompression\n",
    "\n",
    "    :type url_link: str\n",
    "    :param url_link: HTTP link for the archive file\n",
    "\n",
    "    :rtype:\n",
    "        str\n",
    "    \"\"\"\n",
    "    local_file_name = url_link.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url_link, filename=local_file_name)\n",
    "\n",
    "    return local_file_name\n",
    "\n",
    "\n",
    "def extract_tar_archive_file(archive_filename: str) -> str:\n",
    "    \"\"\" Extract content from the archive file and returns\n",
    "    the local folder name\n",
    "\n",
    "    :type archive_filename: str\n",
    "    :param archive_filename: Archive file name\n",
    "\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    # Archive extraction\n",
    "    with tarfile.open(archive_filename) as archive_file:\n",
    "        for member in tqdm(\n",
    "            archive_file.getmembers(),\n",
    "            total=len(archive_file.getmembers()),\n",
    "            desc=\"Archive decompression\"\n",
    "            ):\n",
    "            archive_file.extract(member=member, path='./')\n",
    "\n",
    "\n",
    "local_file = download_file(remote_url_link)\n",
    "local_folder = extract_tar_archive_file(local_file)\n",
    "os.remove(local_file)\n",
    "\n",
    "shutil.move('release2.3.1/original/data/official.sgml','.')\n",
    "shutil.move('release2.3.1/README','.')\n",
    "\n",
    "shutil.rmtree('release2.3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctions marking the end of a sentence\n",
    "PUNCTUATIONS = '.?!'\n",
    "        \n",
    "        \n",
    "\n",
    "def find_end_of_sentence(raw_text:str, character_index:int):\n",
    "    \"\"\" Find the end of the sentence containing a character\n",
    "    :type raw_text: str\n",
    "    :param raw_text: Text containing the sentence of a character\n",
    "\n",
    "    :type character_index: str\n",
    "    :param character_index: Character position\n",
    "\n",
    "    :rtype:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # Selecting only the relevant text\n",
    "    current_text = raw_text[character_index:]\n",
    "    \n",
    "    # Generating the index position of the closest character for each punctuation\n",
    "    punctuation_indexes = {}\n",
    "    for punctuation in PUNCTUATIONS:\n",
    "        try:\n",
    "            punctuation_indexes[punctuation] = current_text.index(punctuation)\n",
    "        # In case it isn't in the text : ignored\n",
    "        except ValueError as _:\n",
    "            pass\n",
    "    \n",
    "    # Adding the default one (in case of no punctuation ending)\n",
    "    punctuation_indexes[''] = len(current_text)\n",
    "    \n",
    "    # Computing the closest punctuation to the character\n",
    "    closest_punctuation = min(punctuation_indexes, key=punctuation_indexes.get)\n",
    "    return punctuation_indexes[closest_punctuation]+character_index\n",
    "\n",
    "def find_beginning_of_sentence(raw_text: str, character_index: str) -> int:\n",
    "    \"\"\" Fetch the beginning of the sentence\n",
    "    :type raw_text: str\n",
    "    :param raw_text: Text containing the sentence\n",
    "\n",
    "    :type character_index: str\n",
    "    :param character_index: Character position in the text\n",
    "\n",
    "    :rtype:\n",
    "        int\n",
    "    \"\"\"\n",
    "    current_index = int(character_index)\n",
    "\n",
    "    # Loop while not having found the beginning of the sentence\n",
    "    while current_index >= 0:\n",
    "        # If the character is an upper case letter\n",
    "        if raw_text[current_index].isupper():\n",
    "            if current_index == 0 or \\\n",
    "                (raw_text[current_index-1] == ' ' and raw_text[current_index-2] in PUNCTUATIONS)\\\n",
    "                    or raw_text[current_index-1] in PUNCTUATIONS:\n",
    "                return current_index\n",
    "        # Begin of the text\n",
    "        if current_index == 0:\n",
    "            return current_index\n",
    "        \n",
    "        # Moves backward\n",
    "        current_index -= 1\n",
    "\n",
    "def extract_sentence(texts:list[str], mistakes:list[dict]) -> list[str]:\n",
    "    \"\"\" Extract from the texts the sentences containing the corrected mistakes\n",
    "    :type texts: list[str]\n",
    "    :param texts: Texts with the mistakes corrected\n",
    "\n",
    "    :type mistakes: list[dict]\n",
    "    :param mistakes: List of every mistakes made\n",
    "\n",
    "    :raises:\n",
    "\n",
    "    :rtype:\n",
    "        Sentences with the corrected mistakes\n",
    "    \"\"\"\n",
    "    # For every sentence\n",
    "    corrected_sentences = []\n",
    "    \n",
    "    # For every mistake\n",
    "    for mistake in mistakes:\n",
    "        # Which text holds the error\n",
    "        part = mistake['part']\n",
    "        \n",
    "        # Correction begin and end indexes\n",
    "        begin = mistake['begin']\n",
    "        end = mistake['end']\n",
    "        \n",
    "        # Select only the relevant text\n",
    "        print(part)\n",
    "        current_text = texts[part]\n",
    "        \n",
    "        # Computing the index surrounding the sentence\n",
    "        sentence_begin_index = find_beginning_of_sentence(current_text, begin)\n",
    "        sentence_end_index = find_end_of_sentence(current_text, end)\n",
    "        \n",
    "        # Appending the sentence to the list\n",
    "        corrected_sentences.append(current_text[sentence_begin_index:sentence_end_index])\n",
    "    \n",
    "    return corrected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     28\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(paragraph) \u001b[38;5;28;01mfor\u001b[39;00m paragraph \u001b[38;5;129;01min\u001b[39;00m doc_text]\n\u001b[1;32m     30\u001b[0m errors_data \u001b[38;5;241m=\u001b[39m [{\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpart\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(raw_error[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_par\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m: (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp_end\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28mint\u001b[39m(raw_error[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_off\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     44\u001b[0m } \u001b[38;5;28;01mfor\u001b[39;00m raw_error \u001b[38;5;129;01min\u001b[39;00m raw_errors]\n\u001b[0;32m---> 46\u001b[0m corrected_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m error_pos, corrected_sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(corrected_sentences):\n\u001b[1;32m     49\u001b[0m     errors[error_pos][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrected_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m corrected_sentence\n",
      "Cell \u001b[0;32mIn[3], line 94\u001b[0m, in \u001b[0;36mextract_sentence\u001b[0;34m(texts, mistakes)\u001b[0m\n\u001b[1;32m     91\u001b[0m current_text \u001b[38;5;241m=\u001b[39m texts[part]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Computing the index surrounding the sentence\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m sentence_begin_index \u001b[38;5;241m=\u001b[39m \u001b[43mfind_beginning_of_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m sentence_end_index \u001b[38;5;241m=\u001b[39m find_end_of_sentence(current_text, end)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Appending the sentence to the list\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mfind_beginning_of_sentence\u001b[0;34m(raw_text, character_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Loop while not having found the beginning of the sentence\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# If the character is an upper case letter\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mraw_text\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misupper():\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m current_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[1;32m     54\u001b[0m             (raw_text[current_index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m raw_text[current_index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m PUNCTUATIONS)\\\n\u001b[1;32m     55\u001b[0m                 \u001b[38;5;129;01mor\u001b[39;00m raw_text[current_index\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m PUNCTUATIONS:\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m current_index\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# Dataframe construction\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Loading data and parsing it\n",
    "with open('official.sgml') as file:\n",
    "    raw_content = file.read()\n",
    "soup = BeautifulSoup(raw_content)\n",
    "\n",
    "# Fetching the documents and the related ids\n",
    "docs = soup.find_all('doc')\n",
    "\n",
    "# Fetching the texts\n",
    "texts = {}\n",
    "for doc in docs:\n",
    "    doc_text = [doc_text.text.replace('\\n', '')\n",
    "                for doc_text in doc.find('text').find_all('p')]\n",
    "    doc_id = doc['nid']\n",
    "\n",
    "    raw_errors = doc.find('annotation').find_all('mistake')\n",
    "\n",
    "    errors = [\n",
    "        {\n",
    "            'mistake_type': raw_error.find('type').text,\n",
    "            'correction_part': raw_error.find('correction').text,\n",
    "        } for raw_error in raw_errors\n",
    "    ]\n",
    "\n",
    "    lengths = [len(paragraph) for paragraph in doc_text]\n",
    "\n",
    "    errors_data = [{\n",
    "        'part': int(raw_error['start_par']),\n",
    "        'begin': (\n",
    "            int(raw_error['start_off'])-sum(lengths[:int(raw_error['start_par'])])\n",
    "            if int(raw_error['start_off']) > sum (lengths[:int(raw_error['start_par'])])\n",
    "            else int(raw_error['start_off'])\n",
    "        ),\n",
    "        'end': (\n",
    "            int(raw_error['end_off'])-sum(lengths[:int(raw_error['start_par'])])\n",
    "            if int(raw_error['end_off']) > sum (lengths[:int(raw_error['start_par'])])\n",
    "            else int(raw_error['end_off'])\n",
    "        ),\n",
    "        'tmp_begin':int(raw_error['start_off']),\n",
    "        'tmp_end':int(raw_error['end_off'])\n",
    "    } for raw_error in raw_errors]\n",
    "\n",
    "    corrected_sentences = extract_sentence(doc_text, errors_data)\n",
    "\n",
    "    for error_pos, corrected_sentence in enumerate(corrected_sentences):\n",
    "        errors[error_pos]['corrected_sentence'] = corrected_sentence\n",
    "\n",
    "    texts[doc_id] = {\n",
    "        'text': doc_text,\n",
    "        'errors': errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
