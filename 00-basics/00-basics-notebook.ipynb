{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar correction - getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was the CoNLL-2013 Shared Task: Grammatical Error Correction.\n",
    "\n",
    "Copyright (C) 2013 Hwee Tou Ng, Joel Tetreault, Siew Mei Wu,\n",
    "                   Yuanbin Wu, Christian Hadiwinoto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workfolder setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "data_folder = 'data'\n",
    "\n",
    "if os.path.exists(data_folder):\n",
    "    shutil.rmtree(data_folder, ignore_errors=True)\n",
    "\n",
    "\n",
    "# Data folder creation\n",
    "os.mkdir(data_folder)\n",
    "os.chdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb8eec1a74943b8911a14d7750558a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Archive decompression:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# URL link for the dataset\n",
    "remote_url_link = \"https://www.comp.nus.edu.sg/~nlp/conll13st/release2.3.1.tar.gz\"\n",
    "\n",
    "\n",
    "def download_file(url_link: str) -> str:\n",
    "    \"\"\" Download the archive file and returns the local filename\n",
    "    for decompression\n",
    "\n",
    "    :type url_link: str\n",
    "    :param url_link: HTTP link for the archive file\n",
    "\n",
    "    :rtype:\n",
    "        str\n",
    "    \"\"\"\n",
    "    local_file_name = url_link.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url_link, filename=local_file_name)\n",
    "\n",
    "    return local_file_name\n",
    "\n",
    "\n",
    "def extract_tar_archive_file(archive_filename: str) -> str:\n",
    "    \"\"\" Extract content from the archive file and returns\n",
    "    the local folder name\n",
    "\n",
    "    :type archive_filename: str\n",
    "    :param archive_filename: Archive file name\n",
    "\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    # Archive extraction\n",
    "    with tarfile.open(archive_filename) as archive_file:\n",
    "        for member in tqdm(\n",
    "            archive_file.getmembers(),\n",
    "            total=len(archive_file.getmembers()),\n",
    "            desc=\"Archive decompression\"\n",
    "            ):\n",
    "            archive_file.extract(member=member, path='./')\n",
    "\n",
    "\n",
    "local_file = download_file(remote_url_link)\n",
    "local_folder = extract_tar_archive_file(local_file)\n",
    "os.remove(local_file)\n",
    "\n",
    "shutil.move('release2.3.1/original/data/official.sgml','.')\n",
    "shutil.move('release2.3.1/README','.')\n",
    "\n",
    "shutil.rmtree('release2.3.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the dataset for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctions marking the end of a sentence\n",
    "PUNCTUATIONS = '.?!'\n",
    "\n",
    "\n",
    "def find_end_of_sentence(raw_text:str, character_index:int):\n",
    "    \"\"\" Find the end of the sentence containing a character\n",
    "    :type raw_text: str\n",
    "    :param raw_text: Text containing the sentence of a character\n",
    "\n",
    "    :type character_index: str\n",
    "    :param character_index: Character position\n",
    "\n",
    "    :rtype:\n",
    "        str\n",
    "    \"\"\"\n",
    "    # Selecting only the relevant text\n",
    "    current_text = raw_text[character_index:]\n",
    "    \n",
    "    # Generating the index position of the closest character for each punctuation\n",
    "    punctuation_indexes = {}\n",
    "    for punctuation in PUNCTUATIONS:\n",
    "        try:\n",
    "            punctuation_indexes[punctuation] = current_text.index(punctuation)\n",
    "        # In case it isn't in the text : ignored\n",
    "        except ValueError as _:\n",
    "            pass\n",
    "    \n",
    "    # Adding the default one (in case of no punctuation ending)\n",
    "    punctuation_indexes[''] = len(current_text)\n",
    "    \n",
    "    # Computing the closest punctuation to the character\n",
    "    closest_punctuation = min(punctuation_indexes, key=punctuation_indexes.get)\n",
    "    return punctuation_indexes[closest_punctuation]+character_index\n",
    "\n",
    "def find_beginning_of_sentence(raw_text: str, character_index: str) -> int:\n",
    "    \"\"\" Fetch the beginning of the sentence\n",
    "    :type raw_text: str\n",
    "    :param raw_text: Text containing the sentence\n",
    "\n",
    "    :type character_index: str\n",
    "    :param character_index: Character position in the text\n",
    "\n",
    "    :rtype:\n",
    "        int\n",
    "    \"\"\"\n",
    "    current_index = int(character_index)\n",
    "\n",
    "    # Loop while not having found the beginning of the sentence\n",
    "    while current_index >= 0:\n",
    "        # If the character is an upper case letter\n",
    "        if raw_text[current_index].isupper():\n",
    "            if current_index == 0 or \\\n",
    "                (raw_text[current_index-1] == ' ' and raw_text[current_index-2] in PUNCTUATIONS)\\\n",
    "                    or raw_text[current_index-1] in PUNCTUATIONS:\n",
    "                return current_index\n",
    "        # Begin of the text\n",
    "        if current_index == 0:\n",
    "            return current_index\n",
    "        \n",
    "        # Moves backward\n",
    "        current_index -= 1\n",
    "\n",
    "def extract_sentence(texts:list[str], mistakes:list[dict]) -> list[str]:\n",
    "    \"\"\" Extract from the texts the sentences containing the corrected mistakes\n",
    "    :type texts: list[str]\n",
    "    :param texts: Texts with the mistakes corrected\n",
    "\n",
    "    :type mistakes: list[dict]\n",
    "    :param mistakes: List of every mistakes made\n",
    "\n",
    "    :raises:\n",
    "\n",
    "    :rtype:\n",
    "        Sentences with the corrected mistakes\n",
    "    \"\"\"\n",
    "    # For every sentence\n",
    "    corrected_sentences = []\n",
    "    \n",
    "    # For every mistake\n",
    "    for mistake in mistakes:\n",
    "        # Which text holds the error\n",
    "        part = mistake['part']\n",
    "        \n",
    "        # Correction begin and end indexes\n",
    "        begin = mistake['begin']\n",
    "        end = mistake['end']\n",
    "        \n",
    "        # Select only the relevant text\n",
    "        print(part)\n",
    "        current_text = texts[part]\n",
    "        \n",
    "        # Computing the index surrounding the sentence\n",
    "        sentence_begin_index = find_beginning_of_sentence(current_text, begin)\n",
    "        sentence_end_index = find_end_of_sentence(current_text, end)\n",
    "        \n",
    "        # Appending the sentence to the list\n",
    "        corrected_sentences.append(current_text[sentence_begin_index:sentence_end_index])\n",
    "    \n",
    "    return corrected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Loading data and parsing it\n",
    "with open('official.sgml') as file:\n",
    "    raw_content = file.read()\n",
    "soup = BeautifulSoup(raw_content)\n",
    "\n",
    "# Fetching the documents and the related ids\n",
    "docs = soup.find_all('doc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data found is formatted inside a `Pandas` dataframe for the model. The tokenization is done since grammar correction occurs not only with grammatically faults but also with other mistakes such as a wrong pronoun used (e.g. `I is a cat` is incorrect. The correct answer should be `I am a cat`).\n",
    "\n",
    "Words have a context built with other words. I choosed to select sentences and not subsentences ending with punctuation like `,` or `;`. They may not include the whole context (e.g. `Yersterday, he ate his apple.`). In the example, the subsentence include only the time context in the first one or the action in the seconde one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Fetching the texts\n",
    "texts = {}\n",
    "for doc in docs:\n",
    "    # Fetching the text and their associated id ; the text is tokenize for grammar correction\n",
    "    doc_text = [word_tokenize(doc_text.text.replace('\\n', '').strip())\n",
    "                for doc_text in doc.find('text').find_all('p')]\n",
    "    doc_id = doc['nid']\n",
    "    \n",
    "    # Size of the paragraphs\n",
    "    lengths = [len(paragraph) for paragraph in doc_text]\n",
    "\n",
    "    # Data for each from the file -> casted for the dataframe \n",
    "    raw_errors = doc.find('annotation').find_all('mistake')\n",
    "    errors = [\n",
    "        {\n",
    "            'part': int(raw_error['start_par']),\n",
    "            'begin': (\n",
    "                int(raw_error['start_off']) -\n",
    "                sum(lengths[:int(raw_error['start_par'])])\n",
    "                if int(raw_error['start_off']) > sum(lengths[:int(raw_error['start_par'])])\n",
    "                else int(raw_error['start_off'])\n",
    "            ),\n",
    "            'end': (\n",
    "                int(raw_error['end_off']) -\n",
    "                sum(lengths[:int(raw_error['start_par'])])\n",
    "                if int(raw_error['end_off']) > sum(lengths[:int(raw_error['start_par'])])\n",
    "                else int(raw_error['end_off'])\n",
    "            ),\n",
    "            'type': raw_error.find('type').text,\n",
    "            'correction': raw_error.find('correction').text,\n",
    "        } for raw_error in raw_errors\n",
    "    ]\n",
    "\n",
    "    # Extracting the sentences from the text for each mistake and corrected answer.\n",
    "    \"\"\"\n",
    "    corrected_sentences = extract_sentence(doc_text, errors_data)\n",
    "\n",
    "    for error_pos, corrected_sentence in enumerate(corrected_sentences):\n",
    "        errors[error_pos]['corrected_sentence'] = corrected_sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # Appending the result for the dataframe\n",
    "    texts[doc_id] = {\n",
    "        'text': doc_text,\n",
    "        'errors_length': len(errors),\n",
    "        'errors': errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rows containing the data\n",
    "text_row = []\n",
    "mistake_types_row = []\n",
    "correction_part_row = []\n",
    "\n",
    "# For each text\n",
    "for doc_id, errors in texts.items():\n",
    "    paragraphs = errors['text']\n",
    "    \n",
    "    # For each mistake\n",
    "    for mistake in errors['errors']:\n",
    "        \n",
    "        # Adding the data inside\n",
    "        text_row.append(\n",
    "            paragraphs[mistake['part']-1]\n",
    "        )\n",
    "        mistake_types_row.append(mistake['type'])\n",
    "        correction_part_row.append(mistake['correction'])\n",
    "\n",
    "# Constructing the dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Raw text':text_row,\n",
    "    'Mistake type':mistake_types_row,\n",
    "    'Corrected part':correction_part_row\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 unique texts are set in the CoNLL-13 dataset.\n",
      "28 grammatical faults exists: ['ArtOrDet' 'Nn' 'Mec' 'Rloc-' 'Vt' 'Wci' 'Um' 'WOadv' 'Trans' 'Ssub' 'V0'\n",
      " 'Prep' 'SVA' 'Vm' 'Vform' 'WOinc' 'Wform' 'Pform' 'Pref' 'Spar' 'Npos'\n",
      " 'Srun' 'Wtone' 'Sfrag' 'Others' 'Smod' 'Wa' 'Cit']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(df['Raw text'].unique())} unique texts are set in the CoNLL-13 dataset.\")\n",
    "print(f\"{len(df['Mistake type'].unique())} grammatical faults exists: {df['Mistake type'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generating random dataframes for train, test and validation\n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "test, val = train_test_split(test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
